{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Natural Language Processing: Predicting Text Sentiment\n",
    "### Big Data Toronto Master Class, June 12, 2012\n",
    "### Nick Pogrebnyakov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and setup\n",
    "\n",
    "Import libraries and download the dataset.\n",
    "\n",
    "The dataset is here: https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "\n",
    "It contains sentences labeled with their sentiment from Amazon, IMDB and Yelp reviews. Each sentence is on a separate line, followed by a tab ('\\t') character and its sentiment: 0 negative, 1 positive. For example:\n",
    "\n",
    "`Wow... Loved this place.\t1`\n",
    "\n",
    "First, import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "import urllib\n",
    "import shutil\n",
    "import re\n",
    "useLocal = False  # use locally saved dataset, rather than download it from the internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, download the dataset, extract files and read the IMDB movie reviews into a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset... done.\n"
     ]
    }
   ],
   "source": [
    "if not useLocal:\n",
    "    print('Downloading dataset...', end = '')\n",
    "    # download the data: we're using the dataset of sentences labeled with sentiment from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "    urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\", \"sentiment.zip\")\n",
    "    print(' done.')\n",
    "\n",
    "    # unzip the file\n",
    "    with zipfile.ZipFile('sentiment.zip', 'r') as z:\n",
    "        z.extractall('sentiment')\n",
    "    \n",
    "# get IMDB movie reviews from the dataset and put them into a list\n",
    "with open(os.path.join('sentiment', 'sentiment labelled sentences', 'imdb_labelled.txt'), 'r') as f:\n",
    "    moviesSet = f.readlines()\n",
    "\n",
    "if not useLocal:\n",
    "    # clean up: remove downloaded files\n",
    "    shutil.rmtree('sentiment')\n",
    "    os.remove('sentiment.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare data for analysis: convert text to features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Convert to lowercase and extract sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a very, very, very slow-moving, aimless movie about a distressed, drifting young man.  ', '0']\n"
     ]
    }
   ],
   "source": [
    "# separate sentiment so we get a list of [sentence, sentiment] for each sentence; also convert to lowercase\n",
    "movies = [m.rstrip('\\n').lower().split('\\t') for m in moviesSet]\n",
    "\n",
    "# print the first item\n",
    "print(movies[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Remove non-alphanumeric characters (commas, brackets etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a very very very slow moving aimless movie about a distressed drifting young man ', '0']\n"
     ]
    }
   ],
   "source": [
    "# remove non-alphanumeric characters\n",
    "movies = [[re.sub(r'\\W+', ' ', m[0]), m[1]] for m in movies]\n",
    "\n",
    "print(movies[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Tokenize the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'very', 'very', 'very', 'slow', 'moving', 'aimless', 'movie', 'about', 'a', 'distressed', 'drifting', 'young', 'man'], '0']\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "movies = [[nltk.word_tokenize(m[0]), m[1]] for m in movies]\n",
    "\n",
    "print(movies[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'veri', 'veri', 'veri', 'slow', 'move', 'aimless', 'movi', 'about', 'a', 'distress', 'drift', 'young', 'man'], '0']\n"
     ]
    }
   ],
   "source": [
    "# stem\n",
    "stemmer = PorterStemmer()\n",
    "movies = [[[stemmer.stem(x) for x in m[0]], m[1]] for m in movies]\n",
    "\n",
    "print(movies[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Convert text to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# create an instance of TfidfVectorizer; get it to remove stopwords\n",
    "tf = TfidfVectorizer(stop_words = 'english', use_idf = False, norm = None, binary = True)\n",
    "\n",
    "# convert text to features\n",
    "x = tf.fit_transform([' '.join(m[0]) for m in movies])\n",
    "\n",
    "# get feature names\n",
    "featureNames = tf.get_feature_names()\n",
    "\n",
    "print(x[0][0, :10].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "y = [int(m[1]) for m in movies]\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.3, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Classifier: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Test prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM prediction accuracy: 0.7633333333333333\n"
     ]
    }
   ],
   "source": [
    "yPred = clf.predict(xTest)\n",
    "accuracy = accuracy_score(yTest, yPred)\n",
    "print(\"SVM prediction accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction accuracy is pretty decent. However, we don't know how the classifier makes its decisions. Which words are associated with positive sentiment and which with negative?\n",
    "\n",
    "To answer this, we'll run a different classifier.\n",
    "\n",
    "#### 3.4. Classifier: random forest\n",
    "\n",
    "Run the random forest and calculate accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy: 0.7166666666666667\n"
     ]
    }
   ],
   "source": [
    "clfRf = RandomForestClassifier(n_estimators = 1000)\n",
    "clfRf.fit(xTrain, yTrain)\n",
    "yPredRf = clfRf.predict(xTest)\n",
    "accuracyRf = accuracy_score(yTest, yPredRf)\n",
    "print(\"Random forest accuracy:\", accuracyRf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is lower than SVM. However, with random forest we can explore which features are actually characteristic of positive and negative sentiment.\n",
    "\n",
    "Extract feature importances from the random forest classifier object and merge it with our list of feature names obtained earlier from `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive features\n",
      "---------------------\n",
      "cool \t 0.7368542739374062\n",
      "actual \t 0.7467717134604411\n",
      "wonder \t 0.770222283047322\n",
      "charismat \t 0.7823763361960124\n",
      "saw \t 0.8316729513725627\n",
      "entertain \t 0.8415735094022428\n",
      "beauti \t 0.848141124950491\n",
      "miss \t 0.8667809851074487\n",
      "soundtrack \t 0.9452088006828642\n",
      "advis \t 0.9965363941374523\n",
      "\n",
      "Top negative features\n",
      "---------------------\n",
      "disappoint \t -1.2436106676999963\n",
      "rate \t -1.154227995398992\n",
      "hate \t -1.0993691644609054\n",
      "bad \t -1.081261851248003\n",
      "ridicul \t -1.0696337521520944\n",
      "plot \t -1.020803928362658\n",
      "suck \t -1.0177033267041526\n",
      "flick \t -0.9451020952533188\n",
      "aw \t -0.9172370268364338\n",
      "worst \t -0.8461390054335775\n"
     ]
    }
   ],
   "source": [
    "featureImportances = list(zip(featureNames, clfRf.feature_importances_))\n",
    "featureImportances = sorted(featureImportances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "coef = clf.coef_[0]\n",
    "\n",
    "# how many top positive and negative features to return\n",
    "topFeatures = 10\n",
    "\n",
    "# sort the coefficients obtained from SVM and get the highest and smallest values\n",
    "sortedCoef = np.argsort(coef)\n",
    "topPos = sortedCoef[-topFeatures:]\n",
    "topNeg = sortedCoef[:topFeatures]\n",
    "\n",
    "print(\"Top positive features\\n---------------------\")\n",
    "for i in topPos:\n",
    "    print(featureNames[i], '\\t', coef[i])\n",
    "    \n",
    "print(\"\\nTop negative features\\n---------------------\")\n",
    "for i in topNeg:\n",
    "    print(featureNames[i], '\\t', coef[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
